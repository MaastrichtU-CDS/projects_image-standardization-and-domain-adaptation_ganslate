import importlib
import inspect
import json
import pkgutil
from pathlib import Path
from typing import Union

import numpy as np
import torch


def mkdirs(*paths):
    for path in paths:
        Path(path).mkdir(parents=True, exist_ok=True)


def make_dataset_of_files(root, extensions):
    """The root of dataset contains files of the given extension."""
    root = Path(root).resolve()
    assert root.is_dir(), f"{root} is not a valid directory"
    paths = [root / file for file in root.iterdir() if has_extension(file, extensions)]
    return sorted(paths)


def make_recursive_dataset_of_files(root, extensions):
    root = Path(root).resolve()
    assert root.is_dir(), f"{root} is not a valid directory"
    paths = []
    for ext in extensions:
        paths.extend(list(root.rglob(f"*{ext}")))
    return sorted(paths)


def has_extension(file, extensions):
    suffix = Path(file).suffixes
    suffix = "".join(suffix)  # join necessary to get ".nii.gz" and similar suffixes properly
    return any(ext in suffix for ext in extensions)


def make_dataset_of_directories(root, extensions):
    """The root of dataset contains folders for each data point. Each data point folder has to have
    (at least) one file of the specified extension. The dataset has to define which file it takes from
    such folder. Useful when using a dataset that stores, for example, an image and a mask together
    in their own folder.
    """
    root = Path(root).resolve()
    assert root.is_dir(), f"{root} is not a valid directory"
    paths = [root / folder for folder in root.iterdir() if (root / folder).is_dir()]
    paths = [folder for folder in paths if has_files_with_extension(folder, extensions)]
    return sorted(paths)


def make_recursive_dataset_of_directories(root, extensions):
    files = make_recursive_dataset_of_files(root, extensions)
    directories = list({f.parent for f in files})
    return directories


def has_files_with_extension(folder, extensions):
    for ext in extensions:
        if not ext.startswith("."):
            ext = "." + ext
        files_in_folder = list(folder.glob(f"*{ext}"))
        if files_in_folder:
            return True
    return False


def find_paths_containing_pattern(path, pattern, recursive=False):
    path = Path(path)
    paths = path.rglob(pattern) if recursive else path.glob(pattern)
    return list(paths)


def load_json(file):
    with open(file, 'r') as f:
        return json.load(f)


def import_class_from_dirs_and_modules(class_name, dirs_modules):
    """Looks for a particular class of `class_name` recursively inside of the
    given modules and directories. The directories that are given to it need
    to have __init__.py in order for this function to recognize its modules."""

    # Goes through the given dirs and modules and gets their absolute paths
    paths = []
    for path in dirs_modules:
        if inspect.ismodule(path):
            dir_path = Path(path.__file__).parent
        else:
            dir_path = Path(path).resolve()
        paths.append(str(dir_path))

    # Goes through the parsed module and dir paths recursevily
    for loader, module_name, _ in pkgutil.walk_packages(paths):
        # Loads a particular module
        module = loader.find_module(module_name).load_module(module_name)
        # Goes through the attributes of the module and checks if it
        # is the one being sought, and if it, returns it.
        for attribute_name in dir(module):
            attribute = getattr(module, attribute_name)
            if inspect.isclass(attribute) and class_name == attribute_name:
                return attribute

    raise ValueError(
        f"Class with name `{class_name}`` not found in any of the given directories or modules. "
        "If it is located in a project folder, set `project_dir` in config as the project's path.")


def reduce(v):
    if isinstance(v, list):
        v = np.array([value.item() if isinstance(value, torch.Tensor) else value for value in v])
    else:
        v = np.array(v)
    return v


def decollate(batch: Union[dict, list]):
    """
    Function to decollate or get individual elements
    from a collated batch generated by Pytorch

    Inputs:
    batch: Batched output, for example, from the Pytorch dataloader
    """
    if isinstance(batch, list):
        batch = [elem[0] if isinstance(elem[0], str) else reduce(elem) for elem in batch]
    elif isinstance(batch, dict):
        batch = {k: v[0] if isinstance(v[0], str) else reduce(v) for k, v in batch.items()}

    return batch
