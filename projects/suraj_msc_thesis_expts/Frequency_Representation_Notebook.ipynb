{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operating-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "import torch.fft as fft\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import monai\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "from modules.losses import mind_loss as mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consolidated-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://discuss.pytorch.org/t/add-label-captions-to-make-grid/42863\n",
    "irange = range\n",
    "\n",
    "def make_grid_with_labels(tensor, labels, nrow=8, limit=20, padding=2,\n",
    "                          normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Make a grid of images.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
    "            or a list of images all of the same size.\n",
    "        labels (list):  ( [labels_1,labels_2,labels_3,...labels_n]) where labels is Bx1 vector of some labels\n",
    "        limit ( int, optional): Limits number of images and labels to make grid of\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
    "            The final grid size is ``(B / nrow, nrow)``. Default: ``8``.\n",
    "        padding (int, optional): amount of padding. Default: ``2``.\n",
    "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
    "            by the min and max values specified by :attr:`range`. Default: ``False``.\n",
    "        range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
    "            then these numbers are used to normalize the image. By default, min and max\n",
    "            are computed from the tensor.\n",
    "        scale_each (bool, optional): If ``True``, scale each image in the batch of\n",
    "            images separately rather than the (min, max) over all images. Default: ``False``.\n",
    "        pad_value (float, optional): Value for the padded pixels. Default: ``0``.\n",
    "\n",
    "    Example:\n",
    "        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n",
    "\n",
    "    \"\"\"\n",
    "    # Opencv configs\n",
    "#     if not isinstance(labels, list):\n",
    "#         raise ValueError\n",
    "#     else:\n",
    "#         labels = np.asarray(labels).T[0]\n",
    "#     if limit is not None:\n",
    "#         tensor = tensor[:limit, ::]\n",
    "#         labels = labels[:limit, ::]\n",
    "\n",
    "    import cv2\n",
    "    font = 1\n",
    "    fontScale = 2\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 1\n",
    "\n",
    "    if not (torch.is_tensor(tensor) or\n",
    "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
    "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n",
    "\n",
    "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
    "    if isinstance(tensor, list):\n",
    "        tensor = torch.stack(tensor, dim=0)\n",
    "\n",
    "    if tensor.dim() == 2:  # single image H x W\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    if tensor.dim() == 3:  # single image\n",
    "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
    "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
    "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
    "\n",
    "    if normalize is True:\n",
    "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "        if range is not None:\n",
    "            assert isinstance(range, tuple), \\\n",
    "                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n",
    "\n",
    "        def norm_ip(img, min, max):\n",
    "            img.clamp_(min=min, max=max)\n",
    "            img.add_(-min).div_(max - min + 1e-5)\n",
    "\n",
    "        def norm_range(t, range):\n",
    "            if range is not None:\n",
    "                norm_ip(t, range[0], range[1])\n",
    "            else:\n",
    "                norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "        if scale_each is True:\n",
    "            for t in tensor:  # loop over mini-batch dimension\n",
    "                norm_range(t, range)\n",
    "        else:\n",
    "            norm_range(tensor, range)\n",
    "\n",
    "    if tensor.size(0) == 1:\n",
    "        return tensor.squeeze(0)\n",
    "\n",
    "    # make the mini-batch of images into a grid\n",
    "    nmaps = tensor.size(0)\n",
    "    xmaps = min(nrow, nmaps)\n",
    "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
    "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
    "    num_channels = tensor.size(1)\n",
    "    grid = tensor.new_full((num_channels, height * ymaps + padding, width * xmaps + padding), pad_value)\n",
    "    k = 0\n",
    "    for y in irange(ymaps):\n",
    "        for x in irange(xmaps):\n",
    "            if k >= nmaps:\n",
    "                break\n",
    "            working_tensor = tensor[k]\n",
    "            if labels is not None:\n",
    "                org = (0, int(tensor[k].shape[1] * 0.3))\n",
    "                working_image = cv2.UMat(\n",
    "                    np.asarray(np.transpose(working_tensor.numpy(), (1, 2, 0)) * 255).astype('uint8'))\n",
    "                image = cv2.putText(working_image, f'{str(labels[k])}', org, font,\n",
    "                                    fontScale, color, thickness, cv2.LINE_AA)\n",
    "                working_tensor = transforms.ToTensor()(image.get())\n",
    "            grid.narrow(1, y * height + padding, height - padding) \\\n",
    "                .narrow(2, x * width + padding, width - padding) \\\n",
    "                .copy_(working_tensor)\n",
    "            k = k + 1\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protective-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization utilities \n",
    "transform_list = [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))]\n",
    "transform = transforms.Compose(transform_list)\n",
    "\n",
    "def show(img, ax):\n",
    "    npimg = img.numpy()\n",
    "    if ax is not None:\n",
    "        ax.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worth-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images and normalization utilities\n",
    "def load_images(fn1, fn2, offset=5):\n",
    "    A = sitk.ReadImage(str(fn1 / 'target.nrrd'))\n",
    "    B = sitk.ReadImage(str(fn2 / 'deformed.nrrd'))\n",
    "\n",
    "    A = sitk.GetArrayFromImage(A)\n",
    "    B = sitk.GetArrayFromImage(B)\n",
    "\n",
    "    A = A[A.shape[0]//2]\n",
    "    B = B[B.shape[0]//2+offset]\n",
    "\n",
    "    A = torch.Tensor(A)\n",
    "    B = torch.Tensor(B)\n",
    "\n",
    "    # Limits the lowest and highest HU unit\n",
    "    A = torch.clamp(A, -1000, 2000)\n",
    "    B = torch.clamp(B, -1000, 2000)\n",
    "\n",
    "    # Normalize Hounsfield units to range [-1,1]\n",
    "    A = min_max_normalize(A, -1000, 2000)\n",
    "    B = min_max_normalize(B, -1000, 2000)\n",
    "\n",
    "    return A, B\n",
    "\n",
    " \n",
    "def min_max_normalize(image, min_value, max_value):\n",
    "    image = image.float()\n",
    "    image = (image - min_value) / (max_value - min_value)\n",
    "    return 2 * image - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unlike-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized Frequency Loss implementation\n",
    "def generalized_freq_loss(fn1, fn2, offset=5):\n",
    "    A, B = load_images(fn1, fn2, offset)\n",
    "    A = (A+1)/2\n",
    "    B = (B+1)/2\n",
    "    \n",
    "    f_A = fft.fftn(A, norm=\"ortho\")\n",
    "    f_B = fft.fftn(B, norm=\"ortho\")\n",
    "    \n",
    "    f_A = fft.fftshift(f_A)\n",
    "    f_B = fft.fftshift(f_B)\n",
    "\n",
    "    f_A = torch.abs(f_A)\n",
    "    f_B = torch.abs(f_B)\n",
    "\n",
    "    f_A = torch.tanh(f_A)\n",
    "    f_B = torch.tanh(f_B)\n",
    "\n",
    "    abs_diff = torch.abs(f_A - f_B)\n",
    "    abs_diff = torch.tanh(abs_diff)\n",
    "    square = torch.square(f_A - f_B)\n",
    "    square = torch.tanh(square)\n",
    "    \n",
    "    images = [A, f_A, B, f_B, abs_diff, square]\n",
    "    for idx, image in enumerate(images):\n",
    "        image = image.unsqueeze(dim=0)\n",
    "        images[idx] = image\n",
    "        \n",
    "    f = plt.figure(figsize=(15, 15))\n",
    "    show(make_grid_with_labels(images, ['A', 'B', 'f_A', 'f_B', 'Abs difference', 'Squared difference']), None)\n",
    "    plt.axis('off')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "marine-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registration Loss implementation\n",
    "def registration_loss(fn1, fn2, offset=0):\n",
    "    f = plt.figure(figsize=(15, 15))\n",
    "\n",
    "    A, B = load_images(fn1, fn2, offset)\n",
    "    \n",
    "    ip = A.unsqueeze(dim=0).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    op = B.unsqueeze(dim=0).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    mil = monai.losses.LocalNormalizedCrossCorrelationLoss(reduction='none')\n",
    "    img =  mil.forward(ip, op)\n",
    "    img = img.squeeze()\n",
    "    img = torch.tanh(img)\n",
    "    img = (img +1)/2\n",
    "    A = (A+1)/2\n",
    "    B = (B+1)/2    \n",
    "    show(make_grid_with_labels([A.unsqueeze(dim=0), B.unsqueeze(dim=0), img.unsqueeze(dim=0)], ['A', 'B', 'Registration Loss']), None)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conceptual-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIND Loss implementation\n",
    "mind_descriptor = mind.MINDDescriptor()\n",
    "def mind_loss(fn1, fn2, offset=0):\n",
    "    f = plt.figure(figsize=(15, 15))\n",
    "    A, B = load_images(fn1, fn2, offset)\n",
    "    input = A.unsqueeze(dim=0).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    target = B.unsqueeze(dim=0).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    input = input.view(-1, 1, *input.shape[3:])\n",
    "    target = target.view(-1, 1, *target.shape[3:])\n",
    "    \n",
    "    ip_feature = mind_descriptor(input)\n",
    "    target_feature = mind_descriptor(target)\n",
    "    \n",
    "\n",
    "    mind_diff = ip_feature - target_feature\n",
    "    l1 = torch.norm(mind_diff, dim=1, keepdim=True).detach()\n",
    "    A = (A+1)/2\n",
    "    B = (B+1)/2   \n",
    "    show(make_grid_with_labels([A.unsqueeze(dim=0), B.unsqueeze(dim=0), l1.squeeze(dim=0)],  ['A', 'B', 'MIND Loss']), None)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "funny-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function to access different losses interactively\n",
    "def get_representations(fn1, fn2, offset, method):\n",
    "    function_call = eval(method)\n",
    "    function_call(fn1, fn2, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sporting-execution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b640961cb242ad95b903ae6deb475c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='offset', max=10), Dropdown(description='method', options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.get_representations(fn1, fn2, offset, method)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive access to loss representations\n",
    "\n",
    "# Set the paths below to a folder containing CBCT image - target.nrrd and CT image - deformed.nrrd, which is obtained post registration\n",
    "fn1 = Path('/work/vq218944/sample_CT_data/0')\n",
    "fn2 = Path('/work/vq218944/sample_CT_data/0')\n",
    "offset = 0\n",
    "\n",
    "interact(get_representations, fn1=fixed(fn1), fn2=fixed(fn2), offset=IntSlider(min=0, max=10), method=['generalized_freq_loss', 'registration_loss', 'mind_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-terrorist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
